# Data Platform
![Screenshot_20240830_150633](https://github.com/user-attachments/assets/ceb0c181-f977-4734-b73c-3d55c318f333)

Data platform that uses Apache Kafka, Apache Flink, ClickHouse and Grafana to collect, monitor and analyze data from a hypothetical set of sensors at electric car charging stations.  

Project carried out for the third year internship of the degree in computer science at the University of Padua.

## Table of Contents

- [Overview](#overview)
- [Docker](#docker)
  * [docker-compose.yml](#docker-composeyml)
  * [run.sh](#runsh)
- [Data Generator and Kafka](#data-generator-and-kafka)
  * [Dockerfile](#dockerfile)
  * [main.py](#mainpy)
  * [Data traffic example](#data-traffic-example)
- [Flink](#flink)
  * [Dockerfile](#dockerfile-1)
  * [/usrlib](#usrlib)
  * [entrypoint.sh](#entrypointsh)
  * [/jobs](#jobs)
    + [KeyedProcessFunction.Context](#keyedprocessfunctioncontext)
    + [KeyedProcessFunction.OnTimerContext](#keyedprocessfunctionontimercontext)
- [ClickHouse](#clickhouse)
  * [init.sql](#initsql)
- [Grafana](#grafana)
  * [/datasources](#datasources)
  * [/dashboards](#dashboards)
- [Security Considerations](#security-considerations)

### Overview
![diagram](https://github.com/user-attachments/assets/14b76538-39c4-47cf-b1b5-99e640123534)

Below is a brief overview of the data platform workflow, from sensor data collection to graphical visualization in Grafana.:
1. Parking sensor, user sensor and charger sensor play the role of producer by sending messages to Kafka inside the topic "charger-station-signals", where they are divided into 100 partitions, one for each charging station;
2. Messages from "charger-station-signals" are consumed simultaneously by ClickHouse and Flink. ClickHouse saves the data exactly as it is so you have a history of all traffic and can use it for very simple queries, while Flink consumes them to perform data aggregations between different sensors and to perform more complex runtime operations;
3. In addition to the original messages, ClickHouse also collects the messages sent by Flink, creating tables for each of them;
4. Grafana is connected with ClickHouse which is recognized as a data source, and now it is possible to query data from the database in real time and visualize it.

### Docker

#### docker-compose.yml

Each configuration for each service has been taken from the official repos, documentations and guides:
- Apache Kafka: one of the examples from the [official repo](https://github.com/apache/kafka/blob/trunk/docker/examples/docker-compose-files/single-node/plaintext/docker-compose.yml);
- Apache Flink: documentation guide for [Docker Compose](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/deployment/resource-providers/standalone/docker/#flink-with-docker-compose) and how to integrate [Flink Python](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker);
- ClickHouse: guide directly from [Docker Hub](https://hub.docker.com/r/clickhouse/clickhouse-server/);
- Grafana: documentation guide on how to use image with [Docker Compose](https://grafana.com/docs/grafana/latest/setup-grafana/installation/docker/#run-grafana-via-docker-compose).

Most services use `user: '${CURRENT_UID}:${CURRENT_GID}'` to grant the user permissions to modify any file on the volume. Their values ​​are taken from the `.env` file, which, if it does not exist, is generated by the `run.sh` file.

#### run.sh

This bash script automates the recreation of the ClickHouse saved data folders, in case they were deleted to clean up the database and logs.
```
dirs=(
    "./clickhouse/var/lib/clickhouse/"
    "./clickhouse/var/log/clickhouse-server/"
)

for dir in ${dirs[@]}; do

    if [ ! -d $dir ]; then
        mkdir -p $dir
    fi
done
```
It also checks if the .env file exists, and if not, gets the UID and GID of the current session and saves them.
```
if [ ! -f ".env" ]; then
    CURRENT_UID=$(id -u)
    CURRENT_GID=$(id -g)
    echo "CURRENT_UID=${CURRENT_UID}" > .env
    echo "CURRENT_GID=${CURRENT_GID}" >> .env
fi
```

Finally, it runs
```
docker compose up -d --build
```

### Data Generator and Kafka

A data generator has been created that simulates the traffic of 3 sensors coming from the charging stations to have data to use and to be able to test the correct functioning of the platform.  

The data generator and Kafka have been put under the same category because all the Kafka configuration has been managed directly on the file that generates the data thanks to the `kafka-python-ng` python library.

#### Dockerfile

It's a simple python image that installs dependencies from the `requirements.txt` file and that will run the `main.py` file.
```
FROM python:3.12-slim

WORKDIR /app
COPY ./data-generator/requirements.txt /app

RUN pip install --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt
```

#### main.py

The main function looks like this.
```
if __name__ == "__main__":

    time.sleep(5)
    kafka_setup()

    time.sleep(5)
    charger_ids = [f'charger-{i}' for i in range(0, 100)]

    threads = []
    for charger_id in charger_ids:
        thread = threading.Thread(target=simulate_charging_station, args=(charger_id,))
        threads.append(thread)
        thread.start()
```

The first thing that is executed after a short pause to allow the Kafka container to start correctly, is its configuration.

```
def kafka_setup():
    admin_client = KafkaAdminClient(bootstrap_servers='broker:19092', client_id='admin1')
    
    charger_station_topic = NewTopic(
        name='charger-station-signals',
        num_partitions=100,
        replication_factor=1
    )

    admin_client.create_topics([charger_station_topic])
    admin_client.close()

    global producer
    producer = KafkaProducer(bootstrap_servers='broker:19092')
```

In particular it connects to the client as admin and creates the topic called `charger-station-signals`. It divides it into exactly 100 partitions, one for each hypothetical charging station, so that the traffic of each charging station is always ensured to be in order.  
Finally it closes the admin client and creates a message producer that will be used throughout the code every time a signal is sent.

Then creates 100 charging station IDs and for each one starts an asynchronous session of the `simulate_charging_station` function.

This function first initializes the 3 classes representing the 3 sensors.
```
def simulate_charging_station(charger_id):

    parking_sensor = ParkingSensor(charger_id)
    user_data_sensor = UserDataSensor(charger_id)
    charger_sensor = ChargerSensor(charger_id)
```

Each of them has a function to initialize the values, to send a signal and possibly functions that simulate a realistic input and that respectively send the signal with the modified fields.  
Here is an example of the parking sensor:
```
class ParkingSensor:
    def __init__(self, charger_id):
        self.timestamp = get_timestamp()
        self.charger_id = charger_id
        self.vehicle_detected = False
        self.plate = None

    def send_ParkingSensor_signal(self):
        data = dict(
            timestamp = self.timestamp,
            charger_id = self.charger_id,
            vehicle_detected = self.vehicle_detected,
            plate = self.plate
        )
        print(f"[+] Sending parking sensor signal:\n {data}\n")
        send_kafka(data)

    def detect_vehicle(self):
        self.timestamp = get_timestamp()
        self.vehicle_detected = True
        self.plate = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))
        self.send_ParkingSensor_signal()
        
    def detect_vehicle_leave(self):
        self.timestamp = get_timestamp()
        self.vehicle_detected = False
        self.send_ParkingSensor_signal()
```

When a signal is sent, a dictionary with all the values ​​is created and passed to the `send_kafka` function, which uses the previously created kafka producer to send the signal to a specific topic and partition.
```
def send_kafka(data):
    producer.send(
    topic='charger-station-signals',
    key=data['charger_id'].encode('utf-8'),
    value=json.dumps(data).encode('utf-8'),
    partition=int(data['charger_id'].split('-')[-1])
)
```

The function `simulate_charging_station` then simulates a hypothetical data traffic coming from these sensors, using a probabilistic component to obtain a random result.  
Scenarios regarding infringements are also generated with a low probability (2%) in order to show how these cases are managed by the platform.
```
    while True:

        time.sleep(1)

        # 30% probability of vehicle detection if no vehicle is detected
        if not parking_sensor.vehicle_detected and probability_check(0.3):
            parking_sensor.detect_vehicle()
            continue

        # 2% probability of vehicle not using the charger
        if parking_sensor.vehicle_detected and probability_check(0.02):
            time.sleep(random.randint(5, 10))
            parking_sensor.detect_vehicle_leave()
            continue

        # 98% probability of vehicle using the charger
        if parking_sensor.vehicle_detected:

            # 2% probability of user waiting too much before using charger
            if probability_check(0.02):
                time.sleep(random.randint(15, 20)) 

            time.sleep(random.randint(1, 3))
            user_data_sensor.connect_user()

            time.sleep(random.randint(1, 3))
            charger_sensor.start_recharging()

            time.sleep(random.randint(20, 40))

            charger_sensor.stop_recharging(user_data_sensor)

            # 2% probability of user not leaving for a while
            if probability_check(0.02):
                time.sleep(random.randint(15, 20))
                parking_sensor.detect_vehicle_leave()
                continue

            # 98% probability of user leaving without extra time
            time.sleep(random.randint(2, 5))
            parking_sensor.detect_vehicle_leave()
            continue
```

Of course a realistic recharge would last about 40 minutes, making the platform test unnecessarily long. The times were therefore rebalanced as follows:
- Realistic session (30min - 50 min):
    - 0s: user arrives at the parking lot;
    - 2m: user identifies himself at the charger;
    - 1m: user connects his vehicle to the charger;
    - 20m-40m: vehicle charging session;
    - 5m: short stop at the end of charging.
- Simulated session (30sec - 50 sec):
    - 0s: user arrives at the parking lot;
    - 2s: user identifies himself at the charger;
    - 1s: user connects his vehicle to the charger;
    - 20s-40s: vehicle charging session;
    - 5s: short stop at the end of charging.

This time is actually a little longer because we must consider the second that is waited for each execution of the function.

#### Data traffic example

Here is an example of possible traffic generated if there was only one charging station and no violations:
```
# The parking sensor recognizes a vehicle that is parked, and then recognizes its license plate.
{'timestamp': '2024-xx-xx xx:58:52', 'charger_id': 'charger-1', 'vehicle_detected': True, 'plate': 'IMF5BXF1'}

# The user connects to the charging station via a specific app or RFID card, the price of EUR/kWh is then specified which varies based on the user's subscription
{'timestamp': '2024-xx-xx xx:58:54', 'charger_id': 'charger-1', 'user_id': 'Z1IYYUMYPI7UW3KF', 'price': 0.59, 'user_connection': True}

# The charging sensor recognizes that the vehicle has been connected to the charging pump and specifies how many kW will be sent every second
{'timestamp': '2024-xx-xx xx:58:55', 'charger_id': 'charger-1', 'recharging': True, 'energy_delivered': 24}

# The charging sensor recognizes that the vehicle has been disconnected from the charging pump
{'timestamp': '2024-xx-xx xx:59:05', 'charger_id': 'charger-1', 'recharging': False, 'energy_delivered': 0}

# The user is logged out once the charging session is finished
{'timestamp': '2024-xx-xx xx:59:05', 'charger_id': 'charger-1', 'user_id': 'Z1IYYUMYPI7UW3KF', 'price': 0.59, 'user_connection': False}

# The parking sensor recognizes when the vehicle has left the parking space
{'timestamp': '2024-xx-xx xx:59:07', 'charger_id': 'charger-1', 'vehicle_detected': False, 'plate': 'IMF5BXF1'}
 ```

### Flink

#### Dockerfile

The flink-jobmanager and flink-taskmanager services use the same Dockerfile that natively supports PyFlink.

```
FROM flink:1.19.1


# install python3 and pip3
RUN apt-get update -y && \
apt-get install -y python3 python3-pip python3-dev && rm -rf /var/lib/apt/lists/*
RUN ln -s /usr/bin/python3 /usr/bin/python

# install PyFlink
RUN pip3 install apache-flink==1.19.1
```

#### /usrlib

As written in the [documentation](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/connectors/datastream/kafka/), Flink’s streaming connectors are not part of the binary distribution, in our case the ones related to Kafka. These dependencies must therefore be downloaded and inserted manually, and this is the purpose of the `usrlib` folder.

#### entrypoint.sh

The `entrypoint.sh` file is used not only to start the jobmanager, but also to run all jobs asynchronously at startup.

```
#!/bin/bash

/opt/flink/bin/jobmanager.sh start-foreground &

sleep 10

python /opt/flink/jobs/transaction_profit.py &
python /opt/flink/jobs/total_occupied.py &
python /opt/flink/jobs/detect_violations.py &
python /opt/flink/jobs/full_sessions.py &

wait
```

#### /jobs

This folder contains all the jobs that will be executed.  

Existing jobs are:
- `full_sessions.py`: it follows each session of each single column from start to finish, extracting all the useful information and aggregating it into a single signal;
- `detect_violations.py`: recognizes scenarios in which a violation is committed and sends a signal specifying the time of the violation, the license plate, if possible the user and the type of violation committed.  
The types of violations currently recognized are:
    - the vehicle left the parking lot without ever having recharged the vehicle;
    - the vehicle remained parked too long before starting to recharge;
    - the vehicle remained in the parking lot too long after recharging.
- `total_occupied.py`: sends a signal every second specifying how many charging stations are currently occupied at that specific timestamp;
- `transaction_profit.py`: it extrapolates from each session how long a vehicle was charging, what its tariff was and how much energy was consumed per second, thus sending data regarding the total energy consumed and the total earnings.

_NOTE: as for `transaction_profit.py`, this job could easily be removed and get the information from a query thanks to the data aggregation of `full_sessions.py`. Instead this job has been kept only as a further example on how it is possible to use Flink, and because it allows to have the exact information every second considering that the signal is sent at the end of the recharging and not at the end of the whole session as it happens for `full_sessions.py`._

Two main methods were used to perform the jobs: 
- `KeyedProcessFunction.Context`: sending the signal when a specific scenario occurs;
- `KeyedProcessFunction.OnTimerContext`: continuously updating the data but sending it only at specific times.

##### KeyedProcessFunction.Context

An example is `full_sessions.py`. Like any job it starts with the initialization of the environment and the import of external dependencies, in our case the Kafka connector present in the `usrlib` folder.  
Specifically, the file was downloaded from [here](https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/3.1.0-1.18/flink-sql-connector-kafka-3.1.0-1.18.jar).
```
env = StreamExecutionEnvironment.get_execution_environment()
env.add_jars("file:///opt/flink/usrlib/flink-sql-connector-kafka-3.2.0-1.19.jar")
```

At this point I create the Kafka consumer, specifying from which topic it will take the signals and to which group it belongs. It is important to specify the group because it allows to consume the same messages of other groups, which maybe need the same signals to obtain different information.  
It is then specified to consume the messages from the beginning and the consumer is added to the stream.

```
kafka_consumer = FlinkKafkaConsumer(
    topics='charger-station-signals',
    deserialization_schema=SimpleStringSchema(),
    properties={'bootstrap.servers': 'broker:19092', 'group.id': 'flink_group_full_sessions'}
)

kafka_consumer.set_start_from_earliest()
stream = env.add_source(kafka_consumer)
```

It is specified that the job must be executed separately for each charging station, that `FullSessions()` will be executed for each of them and that the result will be returned as a string.
```
stream = stream.key_by(lambda x: json.loads(x)['charger_id'])
stream = stream.process(FullSessions(), output_type=Types.STRING())
```

Then the Kafka producer is created that will send messages to the `full_sessions` topic and connected to the stream. Finally the job is executed.
```
kafka_producer = FlinkKafkaProducer(
    topic='full_sessions',
    serialization_schema=SimpleStringSchema(),
    producer_config={'bootstrap.servers': 'broker:19092'}
)

stream.add_sink(kafka_producer)

env.execute()
```
Regarding `FullSessions()`, a class using `KeyedProcessFunction.Context` can have a similar initial structure:
```
class FullSessions(KeyedProcessFunction):

    def __init__(self):
        self.state = None

    def open(self, runtime_context: RuntimeContext):
        self.state = runtime_context.get_state(ValueStateDescriptor(
            "state", Types.PICKLED_BYTE_ARRAY()))
```

The state is what allows the information of each charger to be maintained until they are ready to be sent.  

In fact at the beginning of each execution of the `process_element` function the previous state is retrieved.
```
def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):
    previous_state = self.state.value()
```

After getting data from sensors it is updated.
```
self.state.update(previous_state)
```

When it is ready it can be sent via Kafka producer.
```
return_data = dict(
    charger_id = previous_state['charger_id'],
    ...
    ...
)

yield json.dumps(return_data)
```

It is also possible to clean it to be ready for a new session.
```
self.state.clear()
```

##### KeyedProcessFunction.OnTimerContext

An example is the file `total_occupied.py`.  
The initialization remains very similar to the previous file, although in this case we specify to process all traffic without splitting it based on the charger and to filter the signals to receive only those that contain the key `vehicle_detected`.
```
def only_parking_sensor(data):
    data = json.loads(data)
    if data.get('vehicle_detected') is not None:
        return True
    return False

stream = stream.filter(only_parking_sensor)
stream = stream.key_by(lambda x: 'global')
```

A substantial difference instead is present in the initialization of the class, in which in addition to specifying state you must also specify timer_state, which will store the time value elapsed between one execution and another.
```
class TotalOccupied(KeyedProcessFunction):

    def __init__(self):
        self.state = None
        self.timer_state = None

    def open(self, runtime_context: RuntimeContext):
        self.state = runtime_context.get_state(ValueStateDescriptor(
            "state", Types.PICKLED_BYTE_ARRAY()))
        self.timer_state = runtime_context.get_state(ValueStateDescriptor(
            "timer_state", Types.LONG()))
```

Since we want to run the job once per second, we first initialize the value of timer_state inside `process_element` specifying that it should be executed in the following second.
```
if self.timer_state.value() is None:
    timer = ctx.timestamp() + 1000
    ctx.timer_service().register_processing_time_timer(timer)
    self.timer_state.update(timer)
```

The `on_timer` function, after getting the information from state and sending it through Kafka, proceeds to schedule its next execution the following second, updating timer_state.
```
def on_timer(self, timestamp: int, ctx: 'KeyedProcessFunction.OnTimerContext'):

    result = self.state.value()

    return_data = dict(
        timestamp = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S'),
        occupied = sum(1 for occupied in result.values() if occupied)
    )

    yield json.dumps(return_data)

    timer = ctx.timestamp() + 1000
    ctx.timer_service().register_processing_time_timer(timer)
    self.timer_state.update(timer)
```

### ClickHouse

#### init.sql

It contains all the tables initialized when ClickHouse starts.  
For each topic from which you want to store data in the db in real time, 3 tables are created. Let's take as an example the tables regarding the parking sensor data.  

First I created a table called "data_sensors", which is a special table in ClickHouse designed to consume data directly from a Kafka topic.  
It is specified that the incoming message will be a json string from `charger-station-signals` topic and the parameters `kafka_poll_timeout_ms` and `kafka_max_block_size` are used to tell to ClickHouse to take the data every second.
```
create table if not exists data_sensors (
    json String
) engine = Kafka settings
    kafka_broker_list = 'broker:19092',
    kafka_topic_list = 'charger-station-signals',
    kafka_group_name = 'clickhouse_group',
    kafka_format = 'JSONAsString',
    kafka_poll_timeout_ms = 1000,
    kafka_max_block_size = 1;
```

The `parse_parking_sensor` table will be the final table where the data will be saved and used by Grafana.
```
create table if not exists parse_parking_sensor (
    timestamp DateTime,
    charger_id String,
    vehicle_detected Bool,
    plate String
) ENGINE = MergeTree()
order by timestamp;
```

Finally a materialized view is created which fills `parse_parking_sensor` table with the transformed data.
```
create materialized view if not exists parking_sensor_consumer to parse_parking_sensor as
select
    toDateTime(JSONExtractString(json, 'timestamp')) as timestamp,
    JSONExtractString (json, 'charger_id') as charger_id,
    JSONExtractBool (json, 'vehicle_detected') as vehicle_detected,
    JSONExtractString (json, 'plate') as plate
from data_sensors
WHERE JSONHas(json, 'vehicle_detected');
```

### Grafana

Grafana can be configured both via the GUI and via yaml files, but being inside a container it is more useful to mount the volumes containing the yaml configurations.

#### /datasources
This folder contains all the files related to existing data sources.  
The `datasources.yaml` file allows a very basic and simple configuration of ClickHouse as a data source.  

Note that ClickHouse is not natively recognized by Grafana so the plugin was imported directly into the Docker Compose file using the following parameter:  
`GF_INSTALL_PLUGINS=grafana-clickhouse-datasource`.
```
apiVersion: 1

datasources:
  - name: clickhouse
    type: grafana-clickhouse-datasource
    editable: true
    jsonData:
      host: clickhouse
      port: 8123
      protocol: http
      username: default
      tlsSkipVerify: false
```

#### /dashboards
This folder contains all the configuration files for the various dashboards available.  
The `dashboards.yaml` file is used to simply indicate the folder where the various dashboard json files are located, while `charger-stations-monitor.json` is the configuration file that represents the dashboard.  

This file does not have to be written by hand but is provided by the GUI once the dashboard has been customized to your liking and you want to save it.

### Security Considerations
When evaluating the security of the data platform, a good starting point is to assess which of the top 10 vulnerabilities from the OWASP list may affect the platform and identify ways to prevent them.

#### 1. Broken Access Control
This vulnerability concerns access control, where users might act outside of their intended permissions. In this specific scenario, it is crucial to ensure that only authorized users can access, modify, or view data.  

Best practices include:
- Implementing strict access policies for each component of the platform, granting only the minimum necessary privileges to each role;
- Verifying that all APIs have appropriate access controls;
- Ensuring each level of the platform has a separate and secure authentication system.

#### 2. Cryptographic Failures
Cryptographic issues are particularly important in the data platform as the traffic contains sensitive user information, such as data from charging stations. Proper protection is crucial, especially if this data falls under privacy laws (GDPR) or regulations (PCI DSS).

To address these issues:
- Use up-to-date TLS protocols and inspect all internal traffic;
- Avoid using outdated or weak cryptographic algorithms or protocols by default;
- Do not use default cryptographic keys, weakly generated or reused keys, and avoid placing them in source code repositories;
- Implement security headers in HTTP communication systems, such as Grafana, and use Strict-Transport-Security to enforce TLS;
- Ensure all TLS certificates are signed by a trusted Certificate Authority and that the chain of trust is valid;
- Use key derivation functions to prevent brute-force or rainbow table attacks;
- Check if error messages from Kafka, Flink, or other components could reveal cryptographic information, such as padding errors.

#### 3. Injection
Applications are generally vulnerable to injection when user-provided data is not validated, filtered, or sanitized. In the data platform, even though user input is not required, it is possible that they could be compromised or send malicious data. Therefore, rigorous input validation should be implemented for data from sensors as well.

#### 4. Insecure Design
Insecure Design refers to inadequately or ineffectively designed security controls that cannot be resolved simply by a good implementation.

Guidelines for a data platform include:

- Assessing protection requirements related to data confidentiality, integrity, availability, and authenticity;
- Writing unit and integration tests to ensure critical flows of the platform are secure;
- Implementing a Secure Development Lifecycle (SDLC) across the platform, incorporating security review activities at every project phase.

#### 5. Security Misconfiguration
This risk arises when security configurations are not correctly set or are incomplete, leading to unnecessary or dangerous exposures. Given the multiple components in a data platform, each must be properly configured.

Specifically:
- Develop a hardening process for each platform environment that includes secure configurations for each component, preferably automated to avoid errors;
- Minimize active functionalities in the platform, keeping only necessary components and disabling or removing unused frameworks, modules, ports, and services;
- Ensure each platform component is kept up to date with the latest and most secure versions;
- Ensure the application architecture is segmented to prevent lateral movement in case of an attack, using containerization techniques;
- Implement an automated system to verify security configurations across all environments and detect misalignments or vulnerabilities.

#### 6. Vulnerable and Outdated Components
This vulnerability occurs when software, frameworks, libraries, and other components of an application are outdated, unsupported, or vulnerable.

To address this:
- Automate the identification of unused dependencies;
- Automatically monitor versions and known vulnerabilities in used components;
- Obtain libraries and components from official sources, using secure links and preferably signed packages to avoid compromised or harmful components.

#### 7. Identification and Authentication Failures
Weak authentication and session management vulnerabilities arise when mechanisms for verifying user identities, protecting credentials, and managing sessions are insufficiently secure, leaving the application vulnerable to attacks like credential stuffing, brute force, and other automated unauthorized access attempts.

For the data platform, consider:
- Never deploying systems with default credentials, especially for users with elevated privileges;
- Checking the complexity of user passwords against commonly used and weak passwords;
- Ensuring error messages during registration or login do not reveal verbose information.

#### 8. Software and Data Integrity Failures
Issues with software and data integrity arise when systems do not adequately verify the origin and integrity of code or information. This problem occurs when relying on software components from insecure sources.

To prevent these vulnerabilities:
- Ensure libraries and dependencies come from secure and verified repositories;
- Use tools to continuously monitor dependencies and ensure they do not contain known vulnerabilities;
- Implement review processes for all code and configuration changes.

#### 9. Security Logging and Monitoring Failures
Insufficient logging and monitoring highlight the need for effective systems to detect, track, and respond to security breaches and suspicious activities. Without proper logging and monitoring, detecting ongoing attacks or identifying breaches after they occur becomes challenging.

For the data platform, leverage existing logging systems of various components to:
- Ensure logs are generated in a format compatible with log management systems for automated analysis;
- Implement automatic monitoring systems that can generate real-time alerts when suspicious activity is detected.

#### 10. Server-Side Request Forgery (SSRF)
SSRF vulnerabilities occur when a web application allows a user to control part or all of the URL for a request that is then executed by the server. Without proper input validation, an attacker can exploit this vulnerability to make the server send requests to unintended destinations, such as internal network services or firewall-protected networks.

Since our data platform does not allow users to process or interact directly with external URLs, SSRF vulnerabilities are not applicable in our context and can be excluded from our security considerations.
